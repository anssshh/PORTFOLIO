{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBiiLTH-LTbt"
      },
      "source": [
        "# Load Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS7QszSMyO6o",
        "outputId": "20ae0a57-7fba-4da8-b9c3-4b81035b8012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "  !pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z8I3gr6yHXh",
        "outputId": "ed864908-6e81-47a0-eadc-b5ea77d9b0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMDRQ1agyZ5R",
        "outputId": "3ad0d524-a0e9-4193-c479-a50600e48367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.25.1 rapidfuzz-3.9.3\n"
          ]
        }
      ],
      "source": [
        "pip install fuzzywuzzy python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2-jO4qMyfM_",
        "outputId": "41b6f73a-a6b5-42b3-b5fe-b981ac4a369b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting indonlp\n",
            "  Downloading indoNLP-0.3.4-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: indonlp\n",
            "Successfully installed indonlp-0.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install indonlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRhnbi2fx1_e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#import emoji\n",
        "from sklearn.utils import resample\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtxRdsfWMIYV"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lg1mbuzNFuu"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QgyRk5eONFfL"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3BTHC1QMKQN"
      },
      "source": [
        "# Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJXI1j-hzIjv"
      },
      "outputs": [],
      "source": [
        "# Function to clean data\n",
        "def clean_tweet(tweet):\n",
        "    import re\n",
        "\n",
        "    # Define regex pattern to match usernames, hyperlinks, and old style retweets\n",
        "    pattern = r'(@[\\w/+=]+)|(https?://\\S+)|(\\[RE\\s+\\w+\\])'\n",
        "\n",
        "    # Remove usernames, hyperlinks, and old style retweets\n",
        "    tweet = re.sub(pattern, '', tweet)\n",
        "\n",
        "    # Remove numbers\n",
        "    tweet = re.sub('[0-9]+', '', tweet)\n",
        "\n",
        "    # Remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "    # Menghapus teks retweet gaya lama \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # Remove special characters using regular expressions (re) hashtag\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "# Apply the cleaning function to every row in the 'text' column of the DataFrame\n",
        "df['text'] = df['text'].apply(clean_tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuiARhkYzi4x"
      },
      "outputs": [],
      "source": [
        "#import stopword\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_indonesia = stopwords.words('indonesian')\n",
        "\n",
        "#import sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "#tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Happy Emoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';(', ':\\U0001F92D'\n",
        "    ])\n",
        "\n",
        "# all emoticons (happy + sad)\n",
        "emoticons = emoticons_happy.union(emoticons_sad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVcDox3xzjuC"
      },
      "outputs": [],
      "source": [
        "# Remove emoji\n",
        "def remove_emojis(tweet):\n",
        "    \"\"\"\n",
        "    Removes emojis from text while preserving punctuation, special characters,\n",
        "    and characters from various languages (Korean, Chinese, Russian, German, Spanish, etc.).\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The text to remove emojis from.\n",
        "\n",
        "    Returns:\n",
        "        str: The text with emojis removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unicode emoji character ranges (excluding flags)\n",
        "    emoji_ranges = [\n",
        "        u\"\\U0001F600-\\U0001F64F\",  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\",  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\",  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"   # miscellaneous symbols\n",
        "    ]\n",
        "\n",
        "    # Combine emoji ranges into a single pattern\n",
        "    emoji_pattern = \"|\".join(emoji_ranges)\n",
        "\n",
        "    # Create a pattern that matches any character except letters, numbers, punctuation,\n",
        "    # whitespace, and characters from various languages (excluding flags)\n",
        "    allowed_chars = r\"[a-zA-Z0-9\\s\\-.,!?:;\\(\\)\\[\\]\\{\\}]\"\n",
        "    non_emoji_pattern = f\"{allowed_chars}\"\n",
        "\n",
        "    # Combine the emoji and non-emoji patterns into a single pattern\n",
        "    combined_pattern = f\"(^{non_emoji_pattern})|({emoji_pattern})\"\n",
        "\n",
        "    # Remove emojis while keeping other characters\n",
        "    return re.sub(combined_pattern, r\"\\1\", tweet, flags=re.UNICODE)\n",
        "\n",
        "df['text'] = df['text'].apply(remove_emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05sAFBmjzjpk"
      },
      "outputs": [],
      "source": [
        "#Remove incorrect emoji\n",
        "def incorrect_emoji(tweet):\n",
        "    # Define patterns for incorrect encodings and non-standard characters\n",
        "    encoding_pattern = re.compile(r\"Ã[A-Za-z0-9ÂŒÂˆÂŠÂœ]+\")\n",
        "\n",
        "    # Remove these patterns from the text\n",
        "    tweet = re.sub(encoding_pattern, '', tweet)\n",
        "\n",
        "    # Remove any remaining non-standard characters\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "df['text'] = df['text'].apply(incorrect_emoji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfeqpGdzjoU"
      },
      "outputs": [],
      "source": [
        "#remove duplicate with same label\n",
        "df = df.drop_duplicates(subset=['text', 'label'], keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22hzXsSIz6aJ"
      },
      "outputs": [],
      "source": [
        "#remove duplicate with different label\n",
        "df = df.drop_duplicates(subset=['text'], keep= False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fUIEq-iL0Rux",
        "outputId": "0336fda1-237c-43c3-a06a-7a686494d638"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4231,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4231,\n        \"samples\": [\n          \"Prabowo menyoroti pentingnya pemberdayaan ekonomi daerah dalam visinya MenangkanHatiRakyat KitaSelaluAda UntukPragibs \",\n          \"Kapolri Jenderal Polisi Drs Listyo Sigit Prabowo  menyampaikan dalam sambutannya saat perayaan Natal Mabes Polri tahun  di Auditorium STIKPTIK Lemdiklat Polri bahwa keberagaman yang ada di Indonesia merupakan anugerah dari Tuhan yang harus kita \",\n          \"PDIP buka suara usai Jokowi dan Prabowo makan malam empat mata  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Politik\",\n          \"Ekonomi\",\n          \"Sumber Daya Alam\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8fa232c9-34b2-442d-81d0-0636f1a523ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kunjungan Prabowo ini untuk meresmikan dan men...</td>\n",
              "      <td>Sumber Daya Alam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anies dapat tepuk tangan meriah saat jadi Rekt...</td>\n",
              "      <td>Politik</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>memang benar sih pendukung  ada yang goblok b...</td>\n",
              "      <td>Demografi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sewaktu anies bersikap kritis ke kinerja pak p...</td>\n",
              "      <td>Politik</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Anies Baswedan Harap ASN termasuk TNI dan Polr...</td>\n",
              "      <td>Politik</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fa232c9-34b2-442d-81d0-0636f1a523ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fa232c9-34b2-442d-81d0-0636f1a523ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fa232c9-34b2-442d-81d0-0636f1a523ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-227b7d22-c0d1-4681-be5e-4803fb4558a0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-227b7d22-c0d1-4681-be5e-4803fb4558a0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-227b7d22-c0d1-4681-be5e-4803fb4558a0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text             label\n",
              "0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n",
              "1  Anies dapat tepuk tangan meriah saat jadi Rekt...           Politik\n",
              "2   memang benar sih pendukung  ada yang goblok b...         Demografi\n",
              "3  Sewaktu anies bersikap kritis ke kinerja pak p...           Politik\n",
              "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from indoNLP.preprocessing import replace_slang\n",
        "\n",
        "# Apply replace_slang to the 'text' column\n",
        "df['text'] = df['text'].apply(lambda x: replace_slang(str(x)))\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_TyKlMMz6VF"
      },
      "outputs": [],
      "source": [
        "from indoNLP.preprocessing import replace_word_elongation\n",
        "\n",
        "# Apply replace_word_elongation function to the 'text' column of your DataFrame\n",
        "df['text'] = df['text'].apply(replace_word_elongation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6RH3f8J0ZKF",
        "outputId": "a587bdb1-509b-48cc-fbaa-af466f891975"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Download stopwords for the Indonesian language\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the text\n",
        "df['text'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))  # Convert to lowercase\n",
        "\n",
        "# Remove stopwords from tokens\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Function to remove stopwords from list of words\n",
        "def remove_stopwords(words):\n",
        "    factory = StopWordRemoverFactory()\n",
        "    stopword_remover = factory.create_stop_word_remover()\n",
        "    cleaned_words = stopword_remover.remove(' '.join(words)).split()\n",
        "    return cleaned_words\n",
        "\n",
        "# Apply the remove_stopwords function to each list of words in the 'text' column\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n",
        "\n",
        "# Join the list of words back into a string\n",
        "df['text'] = df['text'].apply(' '.join)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cufrRw4-0dk_"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "df['text'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))  # Convert to lowercase\n",
        "\n",
        "# Remove stopwords from tokens\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYzyPDPE0jp7",
        "outputId": "34ebc676-a665-42cc-e1f0-059f7b883174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most common words:\n",
            "anies 1880\n",
            "ganjar 1760\n",
            "prabowo 1561\n",
            "pranowo 849\n",
            "mahfud 813\n",
            "indonesia 622\n",
            "jnk 598\n",
            "ganjarmahfud 596\n",
            "ganjarpranowopilihanumat 558\n",
            "capres 543\n"
          ]
        }
      ],
      "source": [
        "#find the most use word\n",
        "\n",
        "# Tokenize the text and count word frequencies\n",
        "all_words = ' '.join([' '.join(text) for text in df['text']]).lower()\n",
        "tokens = word_tokenize(all_words)\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Display the most common words\n",
        "most_common_words = word_freq.most_common(10)  # Change 10 to the desired number of most common words\n",
        "print(\"Most common words:\")\n",
        "for word, frequency in most_common_words:\n",
        "    print(word, frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "virsbuFq0Y1l"
      },
      "outputs": [],
      "source": [
        "#Remove kata-kata yang belum terdeteksi\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# List of words to remove\n",
        "words_to_remove = [\n",
        "    'ganjarmahfudrebound', 'md', 'baswedan', 'gibran', 'prabowogibran', 'orang',\n",
        "    'subianto', 'amin', 'ga', 'no', 'amp', 'gm', 'nya', 'imin', 'aniesmuhaimin', 'prof', 'utk', 'lbihbaik',\n",
        "    'beliau', 'abah', 'mas', 'pak', 'aja', 'kalo', 'presiden', 'gak', 'lu', 'gw', 'gue', 'lu', 'sm', 'w', 'doang', 'mah', 'ku', 'goblok',\n",
        "    'retweet', 'reply', 'sih', 'dg', 'sj', 'jg'\n",
        "]\n",
        "\n",
        "# Add words from most_common_words to words_to_remove\n",
        "for word, _ in most_common_words:\n",
        "    words_to_remove.append(word)\n",
        "\n",
        "def remove_specific_words(text):\n",
        "    cleaned_words = []\n",
        "    for word in text:\n",
        "        # Check if the word is in the list of words to remove\n",
        "        if word.lower() in words_to_remove:\n",
        "            continue\n",
        "        cleaned_words.append(word)\n",
        "    return cleaned_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd9W79ma0YyD"
      },
      "outputs": [],
      "source": [
        "# Remove specific words from tokens\n",
        "df['text'] = df['text'].apply(remove_specific_words)\n",
        "\n",
        "# Join the list of words back into a string (if needed)\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vpIfeHzNQv-"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['encoded_label'] = label_encoder.fit_transform(df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XJPuYsKMabX"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "d98da761367b4976ba0631de4279e6de",
            "48163d75f5454338a6a99607b748eb5a",
            "e3cf97fbd88243018d4c2a380bbb3927",
            "59ad5ea0a26a47d8a330a3316ad4e18e",
            "41086b4c507744c4a4c86162e7883d6a",
            "bbe2499b8ee542ce83e0d5923d10f9d8",
            "200b67ed28f3429183aedadba72310c2",
            "96c1bc69d05d4001b8a7a448bc5b060e",
            "712389d5ad6e47cbb023b90b02049653",
            "32a91b95dc3e49798144f5d261376d6d",
            "441872dfd51c4c6297838375c3d2a500",
            "4e8296b44b664f36aa87fee3d3b88b5d",
            "404a0032400c465c9b1920c77ffa7921",
            "9a17597d5da64c9a84a4443934ac1364",
            "2cec0b62aadb41e788992264373dcfd0",
            "17af87006af94a71831d3326c38a2972",
            "2ab20cded17241bcb103b2602d9583fb",
            "455db98cfe60447184bb17621d7f236f",
            "8ba8001fe5394cccbc43ab42e4fa7f1d",
            "d376059c3dec4aa0a0db8ad1c61043eb",
            "426f52d6c6ab487784d08f6e91ab1162",
            "c5cd97689f6d41ae9279a6b832f8dc97",
            "273366b3975348fbac2fb9d3965d12e8",
            "6e9b2bf042914b33b7a53d4d931ee824",
            "92ddf0d953254d7a8853d6cc187700e0",
            "8e7cb73b1d0b401a9bdcd3e0909a45f6",
            "5935533fa829475b83d26f7d70ffa5da",
            "e48f07e086b745b2837123c7b10f92b8",
            "5492abad3eca492facfd9053623568d7",
            "d28abb4a8cb54391ab2db86cbaa31a5d",
            "03143683c521451ba5f17479bf6b54ec",
            "29e01e6b8e9047cb8b34196d6c06b633",
            "677af19ccefd47c8a75055f2cec3da80",
            "394001296530425896c3bebad914776c",
            "7dfabaf010f24957be284f2a7dbcce76",
            "177ca24a630a43cbaf2201fe027254a6",
            "4f4fa7ebf50a491184b4c0a77fe536c4",
            "d28df855f24b4e159553621fc0212131",
            "19a36d878f374205acc1006f216a3480",
            "2c702c2192634234aa678f470b9f1936",
            "e7276263f26c4146aeb60be604d54ea0",
            "e61878102980409a9ae700c6db52e580",
            "57f4212ac2d8426fb7603a89ef82f9a3",
            "9b58ee3f29014730a87f1b065e651a62",
            "0e90f17c1c2a42ac920f89bfe89b5bfb",
            "1c38f40ff8a54cc8ba887cdff098abef",
            "f119053b7f184aa48ee86dc198be5538",
            "58fa95503b024b5384f785c8dd35479d",
            "dd1711eb6b73402da542381c3452d678",
            "eed81839c3234c7fa486688f8cbb820e",
            "f55a4e4ae2e04b4092ebf3219ec0d340",
            "739d6a4e7b45410588331271e3b4bd3e",
            "9b54f2b97b4c4cf9a6921e2838cca0b1",
            "538da850487c4dbb9aa593672c3988e0",
            "bea31e16247c4ff4a1a5bf09e33ae012"
          ]
        },
        "id": "kQuagwnWOo-C",
        "outputId": "51905b47-8579-4295-f8df-b4a645a93065"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_dt, test_dt = train_test_split(df, test_size=0.8, stratify=df['label'], random_state=42)\n",
        "\n",
        "# Balance the training data if it's imbalanced\n",
        "labels = train_dt['label'].value_counts()\n",
        "max_count = labels.max()\n",
        "\n",
        "balanced_train_dts = []\n",
        "\n",
        "for label in labels.index:\n",
        "    label_dt = train_dt[train_dt['label'] == label]\n",
        "    if len(label_dt) < max_count:\n",
        "        label_dt = resample(label_dt, replace=True, n_samples=max_count, random_state=42)\n",
        "    balanced_train_dts.append(label_dt)\n",
        "\n",
        "balanced_train_dt = pd.concat(balanced_train_dts)\n",
        "\n",
        "# Define labels\n",
        "labels_list = [\"Ideologi\", \"Politik\", \"Ekonomi\", \"Sosial Budaya\", \"Pertahanan dan Keamanan\", \"Sumber Daya Alam\", \"Geografi\", \"Demografi\"]\n",
        "\n",
        "# Extract text data as a list of strings\n",
        "train_texts = balanced_train_dt['text'].astype(str).tolist()\n",
        "test_texts = test_dt['text'].astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load IndoBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAPn9CeVW1Cw"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(balanced_train_dt['encoded_label'].tolist())\n",
        "test_labels = torch.tensor(test_dt['encoded_label'].tolist())\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDfAdYHBVkJK",
        "outputId": "abe4f35e-3fbc-40ff-af26-5670ec0ba136"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8HGJKWbAgVt",
        "outputId": "3adc6193-7d18-46f1-9ba0-b00bf9d6f914"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Validation Accuracy: 0.6508\n",
            "Epoch 2/10, Validation Accuracy: 0.7300\n",
            "Epoch 3/10, Validation Accuracy: 0.7258\n",
            "Epoch 4/10, Validation Accuracy: 0.7235\n",
            "Epoch 5/10, Validation Accuracy: 0.7374\n",
            "Epoch 6/10, Validation Accuracy: 0.7400\n",
            "Epoch 7/10, Validation Accuracy: 0.7380\n",
            "Epoch 8/10, Validation Accuracy: 0.7386\n",
            "Epoch 9/10, Validation Accuracy: 0.7386\n",
            "Epoch 10/10, Validation Accuracy: 0.7394\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load model architecture (already done)\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=len(labels))\n",
        "\n",
        "# Define optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Define loss function (CrossEntropyLoss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10  # Example, adjust as needed\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_count = 0\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(logits, dim=1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "        accuracy = balanced_accuracy_score(all_labels, all_predictions)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Validation Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVRWHRwiEUa",
        "outputId": "c530cb40-0217-4a65-cc30-a159b7c0fd84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./model_checkpoint_epoch_9/tokenizer_config.json',\n",
              " './model_checkpoint_epoch_9/special_tokens_map.json',\n",
              " './model_checkpoint_epoch_9/vocab.txt',\n",
              " './model_checkpoint_epoch_9/added_tokens.json')"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model at epoch 9\n",
        "epoch_to_save = 9\n",
        "model_dir = './model_checkpoint_epoch_9'\n",
        "model.save_pretrained(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63MhvcnGiETJ",
        "outputId": "a24bd6ab-7294-4450-c36c-e466ae04bccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.7026\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "#Validation\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "        true_labels.extend(label_ids.flatten())\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
